"""
Model interfaces for the HADES modular pipeline architecture.

This module defines the interfaces for model providers that can be used
for embedding generation and inference.
"""

from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional, Tuple, Union
import numpy as np

from limnos.pipeline.interfaces import Component, Configurable, Pluggable, Serializable


class Model(Component, Configurable, Pluggable, Serializable, ABC):
    """Base interface for all models."""
    
    @property
    def component_type(self) -> str:
        """Return the type of this component."""
        return "model"
    
    @classmethod
    def get_plugin_type(cls) -> str:
        """Return the plugin type for this component."""
        return "model"
    
    @abstractmethod
    def load(self) -> bool:
        """Load the model into memory.
        
        Returns:
            True if loading successful, False otherwise
        """
        pass
    
    @abstractmethod
    def unload(self) -> None:
        """Unload the model from memory."""
        pass
    
    @abstractmethod
    def is_loaded(self) -> bool:
        """Check if the model is loaded.
        
        Returns:
            True if loaded, False otherwise
        """
        pass
    
    @property
    @abstractmethod
    def model_name(self) -> str:
        """Return the name of the model."""
        pass
    
    @property
    @abstractmethod
    def model_provider(self) -> str:
        """Return the provider of the model (e.g., 'ollama', 'openai')."""
        pass


class EmbeddingModel(Model, ABC):
    """Interface for embedding models."""
    
    @property
    def component_type(self) -> str:
        """Return the type of this component."""
        return "embedding_model"
    
    @classmethod
    def get_plugin_type(cls) -> str:
        """Return the plugin type for this component."""
        return "embedding_model"
    
    @abstractmethod
    def embed_documents(self, texts: List[str], batch_size: Optional[int] = None) -> List[np.ndarray]:
        """Generate embeddings for a list of texts.
        
        Args:
            texts: List of texts to embed
            batch_size: Optional batch size for processing
            
        Returns:
            List of embeddings as numpy arrays
        """
        pass
    
    @abstractmethod
    def embed_query(self, query: str) -> np.ndarray:
        """Generate embedding for a query.
        
        Args:
            query: Query text to embed
            
        Returns:
            Embedding as numpy array
        """
        pass
    
    @property
    @abstractmethod
    def embedding_dimension(self) -> int:
        """Return the dimension of the embeddings generated by this model."""
        pass
    
    @abstractmethod
    def warm_up(self) -> None:
        """Warm up the model by running a sample embedding."""
        pass


class InferenceModel(Model, ABC):
    """Interface for inference models."""
    
    @property
    def component_type(self) -> str:
        """Return the type of this component."""
        return "inference_model"
    
    @classmethod
    def get_plugin_type(cls) -> str:
        """Return the plugin type for this component."""
        return "inference_model"
    
    @abstractmethod
    def generate(self, prompt: str, context: Optional[str] = None, 
                max_tokens: Optional[int] = None, 
                temperature: float = 0.7, 
                stop_sequences: Optional[List[str]] = None) -> str:
        """Generate a response for a prompt with optional context.
        
        Args:
            prompt: Prompt text
            context: Optional context to include with the prompt
            max_tokens: Maximum number of tokens to generate
            temperature: Sampling temperature
            stop_sequences: Optional list of sequences that stop generation
            
        Returns:
            Generated text
        """
        pass
    
    @abstractmethod
    def generate_stream(self, prompt: str, context: Optional[str] = None, 
                       max_tokens: Optional[int] = None, 
                       temperature: float = 0.7, 
                       stop_sequences: Optional[List[str]] = None) -> Any:
        """Generate a streaming response for a prompt with optional context.
        
        Args:
            prompt: Prompt text
            context: Optional context to include with the prompt
            max_tokens: Maximum number of tokens to generate
            temperature: Sampling temperature
            stop_sequences: Optional list of sequences that stop generation
            
        Returns:
            Generator yielding text chunks as they're generated
        """
        pass
    
    @abstractmethod
    def tokenize(self, text: str) -> List[int]:
        """Tokenize text using the model's tokenizer.
        
        Args:
            text: Text to tokenize
            
        Returns:
            List of token IDs
        """
        pass
    
    @abstractmethod
    def get_token_count(self, text: str) -> int:
        """Get the number of tokens in the text.
        
        Args:
            text: Text to count tokens for
            
        Returns:
            Number of tokens
        """
        pass
    
    @property
    @abstractmethod
    def max_context_length(self) -> int:
        """Return the maximum context length supported by this model."""
        pass